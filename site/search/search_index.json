{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the documentation webpage of the EDI cluster.</p> Section Description Getting started Basic information about the account setup and connections and data transfer. Running calculations Information on jobs scheduling, SLURM and best practices of interactive work. FAQ Frequently asked questions and solutions to common problems. Available resources Technical description of available resources. Available software List of preinstalled software packages available to all users. Available databases List of databases available to all users. docker running docker on cluster"},{"location":"databases/","title":"Available databases","text":"Name Version Hosts Location Maintainer NCBI NT 2024-02-08 all <code>/home/db/NCBI_NT/*</code> MO NCBI NR90 2023-05-03 all <code>/home/db/NR90/nr90</code> MO"},{"location":"docker/","title":"Docker","text":""},{"location":"docker/#rootless-docker","title":"rootless docker","text":"<p>Docker service is running in rootless mode which is described in detail here: https://docs.docker.com/engine/security/rootless/</p>"},{"location":"docker/#installation","title":"installation","text":"<ol> <li>make sure your user is present in files: <code>/etc/subuid</code> and <code>/etc/subgid</code>. Contact administrator to get entry here</li> <li>run <code>sh /home/nfs/teaching/rootless</code>.</li> <li>add appropraite entries to <code>.bashrc</code>:  <pre><code>export PATH=/home/users/[YOURUSER]/bin:$PATH\n# To get your user id type `id -u`.\nexport DOCKER_HOST=unix:///run/user/[YOUR UID]/docker.sock \n</code></pre>  reload <code>source .bashrc</code> or relogin</li> <li>[Optional] If docker is not available after reloging enable it on startup  <pre><code>systemctl --user enable docker\n</code></pre></li> <li>verify installation <pre><code>docker run -p 8080:80 nginx:alpine\n</code></pre></li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-to-run-jupyter-notebooks-remotely-on-edi","title":"How to run Jupyter notebooks remotely on EDI?","text":"<p>Assuming <code>jupyter</code> is running on <code>edi07</code> (port <code>8888</code>)  and that the connection is established with <code>sshuttle</code> (see Getting started section for more details) it is possible setup the tunnel as follows: <pre><code>ssh -NL 8888:localhost:8888 your_username@edi07\n</code></pre></p> <p>Afterwards you should be able to see the running Jupyter instance via browser at the URL: <code>http://localhost:8888</code></p>"},{"location":"faq/#how-to-install-python-packages-on-edi","title":"How to install python packages on EDI?","text":"<p><code>python3</code> (3.6.9) and <code>python2</code> (2.7.17) along with the recent <code>pip</code>  and <code>virtualenv</code> are installed system-wide on each EDI.</p> <p>You can simply install packages either with <code>pip install --user</code> or use <code>virtualenv</code> (this will allow to handle multiple projects with possibly conflicting dependencies).</p> <p>Finally, if you need newer versions of <code>python</code> or want to handle complicated dependencies you can install a local version of <code>anaconda</code> in your <code>$HOME</code> directory.</p>"},{"location":"faq/#i-want-to-use-program-xxx-on-edi-can-i-install-it-on-my-own","title":"I want to use program <code>XXX</code> on EDI, can I install it on my own?","text":"<p>You can install any software you like in your <code>$HOME</code> directory (as long as you have a valid license to use it).</p> <p>If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.</p>"},{"location":"faq/#what-are-the-guidelines-for-homenfs-distributed-filesystem-use","title":"What are the guidelines for <code>/home/nfs/</code> distributed filesystem use?","text":"<ul> <li>Store only the necessary data in your <code>/home/nfs/your_username</code> directory. The space in this filesystem is limited.<ul> <li>For example: storing your conda distribution, scripts, inputs for jobs run on the cluster and small outputs that you'll move to personal space after completion of the calculations is OK. </li> </ul> </li> <li>Avoid prolonged and heavy I/O operations (like reading and writing large files).<ul> <li>For example: Writing output from the MD simulations in <code>amber</code> (successive, small write operations) is OK. Reading a 100 GB file from <code>nfs</code> directory is not OK.</li> </ul> </li> </ul>"},{"location":"first_steps/","title":"Getting started","text":""},{"location":"first_steps/#setting-up-an-account","title":"Setting up an account","text":"<p>In order to create your an account on EDI cluster please reach out to Kamil and / or Staszek via Slack or e-mail.</p> <p>** After the account is approved you will receive credentials via e-mail from the <code> it @ cent.uw.edu.pl </code> address.**</p> <p>The obtained password will allow to login to the entry node (jumphost) at <code>lbs.cent.uw.edu.pl</code> and compute nodes <code>edi0[0-8]</code>.</p> <p>** Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK **</p> <p>Warning</p> <p>Important: in case of lost password or other technical difficulties related to the entry node  (not compute nodes) please reach out to the IT department at CeNT UW - address: <code>it @ cent.uw.edu.pl</code>.</p> <p>Include the <code>[sih-61]</code> prefix in the message title and add cluster administrators @Kamil and @Staszek in CC.</p> <p>Information</p> <p>Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login.</p>"},{"location":"first_steps/#connecting-via-ssh","title":"Connecting via SSH","text":"<p>Connections to the EDI cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: </p> <p>In order to login to the entry node you can issue the following command:</p> <p><pre><code>ssh your_username@lbs.cent.uw.edu.pl\n</code></pre> This will bring you to the entry node (jumphost), afterwards you can connect to any of the compute nodes  (click here for a complete list of available resources), for example: <pre><code>ssh your_username@edi01\n</code></pre></p> <p>In order to simplify file copying, every day work with e.g. Jupyter notebooks the suggested way of connecting to individual <code>edi</code> nodes is to use sshuttle. This allows to bypass the login node and work almost the same way as being connected via VPN to the local network.</p> <p>Example</p> <p>Assuming <code>sshuttle</code> was installed according to the guide you can connect as follows: <pre><code>sshuttle --dns -NHr your_username@lbs.cent.uw.edu.pl 10.10.61.1/24\n</code></pre> Once connection is established you can directly login to any <code>edi</code> node: <pre><code>ssh edi05\n</code></pre></p> <p>Information</p> <p>Additionally, depending on your computer and network settings, you may have to connect to <code>edi</code> nodes  once without <code>sshuttle</code> so that SSH connections are properly configured.</p> <p>To avoid putting password during each login you can set up authorization via a certificate - additional information is available here</p>"},{"location":"first_steps/#work-environment","title":"Work environment","text":"<p>Each user has access to two personal directories:</p> <ul> <li><code>/home/users/your_username</code></li> <li><code>/home/nfs/your_username</code></li> </ul> <p>Warning</p> <p>The contents of the default directory <code>/home/users/your_username</code> are unique to each compute node and  they are not available on other nodes. </p> <p>In order to use distributed file system please <code>/home/nfs/your_username</code> network mount  (please follow the guidelines).</p>"},{"location":"first_steps/#transferring-files","title":"Transferring files","text":"<p>The recommended options to send or fetch files from EDI cluster are either <code>scp</code> or <code>rsync</code>.</p> <p>The storage on the entry host <code>lbs.cent.uw.edu.pl</code> is very limited therefore it is recommended to setup sshuttle to send / fetch files directly.</p> <p>Information</p> <p>Assuming you established a connection with <code>sshuttle</code> you can directly send files or directories to any <code>edi</code> node: <pre><code>scp file.txt your_username@edi05:\n</code></pre></p>"},{"location":"first_steps/#jupyter-setup","title":"Jupyter setup","text":""},{"location":"first_steps/#add-entry-to-ssh-config-file","title":"add entry to ssh config file","text":"<p>With below entry you can login directly into edi cluster eg. <code>ssh user@ediXX</code> without specification of the jumphost. add the following entry to your <code>~/.ssh/config</code> if the file does not exsits create it. REPLACE <code>USER</code> with your user name and <code>ediXX</code> with appropraite node in below command. <pre><code>Host jumphost\nUser USER\nHostname lbs.cent.uw.edu.pl\nHost ediXX\nUser USER\nProxyJump jumphost\n</code></pre></p>"},{"location":"first_steps/#start-jupyter-instance","title":"start jupyter instance","text":"<p>now login into desired node activate appropriate python environemnt. REPLACE <code>port</code> with you port typically in the range of 8000. <pre><code>jupyter-lab --no-browser --port [port]\n</code></pre></p>"},{"location":"first_steps/#forward-port","title":"forward port","text":"<p>REPLACE <code>[port]</code> with the port value from above command and <code>ediXX</code> with destination node. <pre><code>ssh -NL [port]:localhost:[port] ediXX\n</code></pre></p>"},{"location":"first_steps/#next-steps","title":"Next steps","text":"<p>Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.</p>"},{"location":"resources/","title":"Available resources","text":"<p>EDI cluster has in total 92 physical CPUs (184 virtual), 16 GPUs, 576 GB of RAM and around 140 TB of storage.</p> <p>Characteristcs of available compute nodes are summarized below:</p> Name CPU GPU RAM Storage edi00 Xeon Gold 6230 (20 cores) None 128 GB 8 x 6TB (RAID5) HDD 1 TB SSD edi0[1-3] i7-6950X  (10 cores) 2x GeForce 1080  (8 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi0[4-7] i9-9900K  (8 cores) 2x NVIDIA RTX 2080Ti  (11 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi08 i9-9900K  (8 cores) 2x NVIDIA RTX A4500   (20 GB VRAM) 64 GB 4 x 6TB HDD (RAID5)"},{"location":"rules/","title":"General rules","text":"<p>** Currently available only in Polish. English version coming soon. **</p> <p>Poni\u017csze zasady dotycz\u0105 has\u0142a do w\u0119z\u0142a dost\u0119powego <code>lbs.cent.uw.edu.pl</code>.</p> <ul> <li>U\u017cytkownik jest zobowi\u0105zany do ochrony swojego has\u0142a.</li> <li>Has\u0142o ma charakter poufny. Zabronione jest udost\u0119pnianie ujawnianie has\u0142a w jakikolwiek spos\u00f3b (np. przekazanie innej osobie, zapisywanie w formie nie szyfrowanej, pozostawianie na widoku).</li> <li>W przypadku podejrzenia ujawnienia has\u0142a u\u017cytkownik zobowi\u0105zany jest do jego natychmiastowej zmiany oraz przekazania informacji o zdarzeniu do administrator\u00f3w klastra oraz Dzia\u0142u IT CeNT UW.</li> <li>U\u017cytkownik ponosi odpowiedzialno\u015b\u0107 za wszelkie czynno\u015bci wykonane za po\u015brednictwem konta do niego przypisanego zabezpieczonego jego has\u0142em.</li> <li>Tworz\u0105c has\u0142o nale\u017cy unika\u0107: przyjmowania regularnych schemat\u00f3w w tworzeniu hase\u0142, wybierania sekwencji b\u0105d\u017a powt\u00f3rze\u0144 znak\u00f3w \u0142atwych do podejrzenia, korzystania z nazwy w\u0142asnej u\u017cytkownika, wa\u017cnych dla u\u017cytkownika dat, imion, numer\u00f3w telefon\u00f3w kom\u00f3rkowych, numer\u00f3w rejestracyjnych aut.</li> <li>Zasady tworzenia hase\u0142: Has\u0142o powinno mie\u0107 co najmniej 10 znak\u00f3w (zalecane minimum 15), has\u0142o powinno zawiera\u0107 wielkie i ma\u0142e litery, znaki specjalne, cyfry (0 - 9).</li> <li>Zabrania si\u0119: podawania has\u0142a w wiadomo\u015bciach e-mail, b\u0105d\u017a te\u017c w odpowiedzi na \u017c\u0105danie, kt\u00f3re zosta\u0142o przes\u0142ane poczt\u0105 e-mail, wpisywania hase\u0142 do komputer\u00f3w powstaj\u0105cych w u\u017cytku publicznym np. komputery w kafejkach internetowych, hotelach, bibliotekach itd, zapisywania hase\u0142 w plikach niezaszyfrowanych.</li> <li>U\u017cytkownik zobligowany jest do zmiany has\u0142a nadanego przez administratora przy pierwszej pr\u00f3bie logowania.</li> <li>Zmiana has\u0142a dost\u0119powego do w\u0119z\u0142a dost\u0119powego wymuszana jest co 90 dni.</li> <li>Informacje o utracie has\u0142a, konieczno\u015bci jego zresetowania oraz inne problemy techniczne powinny by\u0107 zg\u0142aszane na adres dzia\u0142u IT CeNT UW - <code>it @ cent.uw.edu.pl </code> do\u0142\u0105czaj\u0105c w CC administrator\u00f3w klasta. Temat wiadomo\u015bci powinien rozpoczyna\u0107 si\u0119 od prefiksu <code>[sih-61]</code>.</li> </ul>"},{"location":"running_jobs/","title":"Running calculations","text":""},{"location":"running_jobs/#introduction","title":"Introduction","text":"<p>Jobs (both batch and interactive sessions) on EDI should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link</p> <p>Information</p> <p>Slurm details:</p> <ul> <li>Two partitions are available - <code>cpu</code> and <code>gpu</code>.</li> <li>GPU partition has higher priority.</li> <li>No limits are currently enforced on the time of execution.</li> <li>Constraints (<code>rtx2080</code>, <code>gtx1080</code>) can be used to select certain GPU architectures.</li> </ul> <p>Example</p> <p>To get the information on the currently running jobs run <code>squeue</code>: <pre><code>~$ squeue \nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n87719       gpu interact username  R 11-18:07:21      1 edi08\n</code></pre> To get the information on the slurm partitions and their details run <code>sinfo</code>: <pre><code>~$ sinfo \nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncpu          up   infinite      1 drain* edi03\ncpu          up   infinite      1    mix edi08\ncpu          up   infinite      7   idle edi[00-02,04-07]\ngpu          up   infinite      1 drain* edi03\ngpu          up   infinite      1    mix edi08\ngpu          up   infinite      6   idle edi[01-02,04-07]\n</code></pre></p>"},{"location":"running_jobs/#interactive-sessions","title":"Interactive sessions","text":"<p>EDI is commonly used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks.  To facilitate allocating resources for interactive sessions a convenient wrapper (<code>alloc</code>) has been prepared. You can tweak your allocation depending on work needs, see the following table for details and examples.</p> <p><code>alloc</code> options are as follows:</p> Argument Description -n Number of cores used allocated for the job (default = 1, max = 36) -g Number of GPUs allocated for the job (default = 0, max = 2) -m Amount of memory (in GBs) per allocated core allocated for the job (default = 1, max = 60) -w Host to start your session (default = host you are running alloc on) <p>Example</p> <p>To obtain an allocation on edi02 with 1 gpu and 6 cores and a total of 12 GB of memory: <pre><code>alloc -n 6 -w edi02 -g 1 -m 2\n</code></pre> Important! Please remember to quit your interactive allocation when you're done with your work.</p>"},{"location":"running_jobs/#batch-jobs","title":"Batch jobs","text":"<p>Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job:</p> <p>Example</p> <p>Suppose the following <code>job.sh</code> batch file: <pre><code>#!/bin/bash\n#SBATCH -p gpu          # GPU partition\n#SBATCH -n 8            # 8 cores\n#SBATCH --gres=gpu:1    # 1 GPU \n#SBATCH --mem=30GB      # 30 GB of RAM\n#SBATCH -J job_name     # name of your job\n\nyour_program -i input_file -o output_path\n</code></pre> You can submit the specified job via <code>sbatch</code> command: <pre><code>~$ sbatch job.sh\nSubmitted batch job 1234\n</code></pre></p>"},{"location":"running_jobs/#array-jobs","title":"Array jobs","text":"<p>This mechamism allows to run multiple similar jobs in batch-like fashion. </p> <p>Example</p> <p>Suppose the following <code>job.slurm</code> file: <pre><code>#!/bin/bash\n\n#SBATCH -p cpu # cpu partition\n#SBATCH -J name of the array jon\n# exclude nodes without gpu\n#SBATCH --exclude=edi[00,01,07,08] # nodes exluded from calculations\n\n#SBATCH --array=1-20%4 # declare a set of 20 jobs, where each 4 jobs running concurently\n#SBATCH --cpus-per-task=4 # number of cores per each task\n#SBATCH --mem=8GB # memory per each task\n#SBATCH --output=%A_%a.out # output of each single job in form of JOBID_TASKID\n#SBATCH --error=%A_%a.err # similar as above\n\n#SBATCH --chdir=... directory where your job will be submit which is optional\n</code></pre></p> <p>slurm provide useful enviroment variables to manage each task parametrization</p> <p><code>SLURM_ARRAY_TASK_ID</code> - number of current task in our example varing from 1 to 20 <code>SLURM_ARRAY_JOB_ID</code> - id of the array job</p> <p>Array job can be submit in the same way as batch jobs.</p> <p>More informations: https://slurm.schedmd.com/job_array.html</p>"},{"location":"software/","title":"Available software","text":"Name Version Hosts Location Maintainer amber  (with ambertools) 18 all <code>/opt/apps/amber18/*</code> KK-SDH amber  (with ambertools) 20 all <code>/opt/apps/amber20/*</code> KK-SDH blast-legacy 2.2.9 all <code>/opt/apps/blast-legacy/bin/*</code> KK-SDH cd-hit 4.8.1 all <code>/opt/apps/cd-hit </code> KK-SDH ccmpred 688e4ba all <code>/opt/apps/CCMpred/bin/ccmpred </code> KK-SDH clustalomega 1.2.4 all <code>/opt/apps/clustalo/clustalo-1.2.4 </code> KK-SDH dorado 0.3.3 all <code>/opt/apps/dorado-0.3.3-linux-x64/bin/</code> KK-SDH dssp 3.1.4 all <code>/opt/apps/dssp-3.1.4/mkdssp</code>  Binary erroneously shows version 3.1.2 KK-SDH dssp 2.3.0 all <code>/opt/apps/dssp-2.3.0/mkdssp</code>  This version works with Socket KK-SDH hh-suite 3.3.0 all <code>/opt/apps/hh-suite/bin/*</code> KK-SDH hmmer 3.3.2 all <code>/opt/apps/hmmer-3.3.2/*</code> KK-SDH mafft 7.490 all <code>/opt/apps/mafft/bin/* </code> KK-SDH master 1.5.1 all <code>/opt/apps/master/bin/*</code> KK-SDH maxcluster 0.6.6 all <code>/opt/apps//maxcluster/maxcluster64bit</code> KK-SDH mmseqs2 9-d36de all <code>/opt/apps/MMseqs2/bin/* </code> KK-SDH modeller 9.2.5 all <code>mod9.25</code> (in <code>$PATH</code>) SDH muscle 3.8.31 all <code>/opt/apps/muscle/muscle3.8.31 </code> KK-SDH ncbi-blast 2.9.0 all <code>/opt/apps/ncbi-blast+/bin/* </code> KK-SDH ont-guppy 6.0.1+652ffd1 <code>edi0[1-8]</code> <code>/opt/apps/ont-guppy/bin/* </code> KK-SDH pd2_public 5b84579 all <code>/opt/apps/pd2_public </code> KK-SDH pymol 2.3.0 all <code>/opt/apps/pymol/bin/pymol </code> KK-SDH raxml 1.2.0 all <code>/opt/apps/raxmlng-1.2.0/bin/</code> KK-SDH rosetta 3.10 all <code>/opt/apps/rosetta-3.10/main/source/bin/* </code> KK-SDH rosetta 3.12 all <code>/opt/apps/rosetta-3.12/main/source/bin/* </code> KK-SDH socket 3.03 all <code>/opt/apps/socket/socket</code> KK-SDH socket2 all all <code>/opt/apps/socket2/socket2</code>  Binary erroneously shows version 3.02 KK-SDH"}]}