{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation webpage of the EDI cluster. Section Description Getting started Basic information about the account setup and connections and data transfer. Running calculations Information on jobs scheduling, SLURM and best practices of interactive work. FAQ Frequently asked questions and solutions to common problems. Available resources Technical description of available resources. Available software List of preinstalled software packages available to all users.","title":"Home"},{"location":"faq/","text":"How to run Jupyter notebooks remotely on EDI? Assuming jupyter is running on edi07 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@edi07 Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888 How to install python packages on EDI? python3 (3.6.9) and python2 (2.7.17) along with the recent pip and virtualenv are installed system-wide on each EDI. You can simply install packages either with pip install --user or use virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory. I want to use program XXX on EDI, can I install it on my own? You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators. What are the guidelines for /home/nfs/ distributed filesystem use? Store only the necessary data in your /home/nfs/your_username directory. The space in this filesystem is limited. For example: storing your conda distribution, scripts, inputs for jobs run on the cluster and small outputs that you'll move to personal space after completion of the calculations is OK . Avoid prolonged and heavy I/O operations (like reading and writing large files). For example: Writing output from the MD simulations in amber (successive, small write operations) is OK . Reading a 100 GB file from nfs directory is not OK .","title":"FAQ"},{"location":"faq/#how-to-run-jupyter-notebooks-remotely-on-edi","text":"Assuming jupyter is running on edi07 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@edi07 Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888","title":"How to run Jupyter notebooks remotely on EDI?"},{"location":"faq/#how-to-install-python-packages-on-edi","text":"python3 (3.6.9) and python2 (2.7.17) along with the recent pip and virtualenv are installed system-wide on each EDI. You can simply install packages either with pip install --user or use virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory.","title":"How to install python packages on EDI?"},{"location":"faq/#i-want-to-use-program-xxx-on-edi-can-i-install-it-on-my-own","text":"You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"I want to use program XXX on EDI, can I install it on my own?"},{"location":"faq/#what-are-the-guidelines-for-homenfs-distributed-filesystem-use","text":"Store only the necessary data in your /home/nfs/your_username directory. The space in this filesystem is limited. For example: storing your conda distribution, scripts, inputs for jobs run on the cluster and small outputs that you'll move to personal space after completion of the calculations is OK . Avoid prolonged and heavy I/O operations (like reading and writing large files). For example: Writing output from the MD simulations in amber (successive, small write operations) is OK . Reading a 100 GB file from nfs directory is not OK .","title":"What are the guidelines for /home/nfs/ distributed filesystem use?"},{"location":"first_steps/","text":"Setting up an account In order to create your an account on EDI cluster please reach out to Kamil and / or Staszek via Slack or e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow to login to the entry node (jumphost) at lbs.cent.uw.edu.pl and compute nodes edi0[0-8] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-61] prefix in the message title and add cluster administrators @Kamil and @Staszek in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login. Connecting via SSH Connections to the EDI cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@lbs.cent.uw.edu.pl This will bring you to the entry node (jumphost) , afterwards you can connect to any of the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@edi01 In order to simplify file copying, every day work with e.g. Jupyter notebooks the suggested way of connecting to individual edi nodes is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@lbs.cent.uw.edu.pl 10 .10.61.1/24 Once connection is established you can directly login to any edi node: ssh edi05 Information Additionally, depending on your computer and network settings, you may have to connect to edi nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here Work environment Each user has access to two personal directories: /home/users/your_username /home/nfs/your_username Warning The contents of the default directory /home/users/your_username are unique to each compute node and they are not available on other nodes. In order to use distributed file system please /home/nfs/your_username network mount (please follow the guidelines ). Transferring files The recommended options to send or fetch files from EDI cluster are either scp or rsync . The storage on the entry host lbs.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly. Information Assuming you established a connection with sshuttle you can directly send files or directories to any edi node: scp file.txt your_username@edi05: Next steps Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Getting started"},{"location":"first_steps/#setting-up-an-account","text":"In order to create your an account on EDI cluster please reach out to Kamil and / or Staszek via Slack or e-mail. After the account is approved you will receive credentials via e-mail from the it @ cent.uw.edu.pl address. The obtained password will allow to login to the entry node (jumphost) at lbs.cent.uw.edu.pl and compute nodes edi0[0-8] . Please familiarize yourself with the general rules of cluster usage before proceeding further - LINK Warning Important: in case of lost password or other technical difficulties related to the entry node (not compute nodes) please reach out to the IT department at CeNT UW - address: it @ cent.uw.edu.pl . Include the [sih-61] prefix in the message title and add cluster administrators @Kamil and @Staszek in CC. Information Please note that password changes on each of the compute nodes and the entry node are synced. It is advised to change your initially obtained password after first login.","title":"Setting up an account"},{"location":"first_steps/#connecting-via-ssh","text":"Connections to the EDI cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@lbs.cent.uw.edu.pl This will bring you to the entry node (jumphost) , afterwards you can connect to any of the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@edi01 In order to simplify file copying, every day work with e.g. Jupyter notebooks the suggested way of connecting to individual edi nodes is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@lbs.cent.uw.edu.pl 10 .10.61.1/24 Once connection is established you can directly login to any edi node: ssh edi05 Information Additionally, depending on your computer and network settings, you may have to connect to edi nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here","title":"Connecting via SSH"},{"location":"first_steps/#work-environment","text":"Each user has access to two personal directories: /home/users/your_username /home/nfs/your_username Warning The contents of the default directory /home/users/your_username are unique to each compute node and they are not available on other nodes. In order to use distributed file system please /home/nfs/your_username network mount (please follow the guidelines ).","title":"Work environment"},{"location":"first_steps/#transferring-files","text":"The recommended options to send or fetch files from EDI cluster are either scp or rsync . The storage on the entry host lbs.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly. Information Assuming you established a connection with sshuttle you can directly send files or directories to any edi node: scp file.txt your_username@edi05:","title":"Transferring files"},{"location":"first_steps/#next-steps","text":"Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Next steps"},{"location":"resources/","text":"EDI cluster has in total 92 physical CPUs (184 virtual), 16 GPUs, 576 GB of RAM and around 140 TB of storage. Characteristcs of available compute nodes are summarized below: Name CPU GPU RAM Storage edi00 Xeon Gold 6230 (20 cores) None 128 GB 8 x 6TB (RAID5) HDD 1 TB SSD edi0[1-3] i7-6950X (10 cores) 2x GeForce 1080 (8 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi0[4-7] i9-9900K (8 cores) 2x NVIDIA RTX 2080Ti (11 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi08 i9-9900K (8 cores) 2x NVIDIA RTX A4500 (20 GB VRAM) 64 GB 4 x 6TB HDD (RAID5)","title":"Available resources"},{"location":"rules/","text":"Currently available only in Polish. English version coming soon. Poni\u017csze zasady dotycz\u0105 has\u0142a do w\u0119z\u0142a dost\u0119powego lbs.cent.uw.edu.pl . U\u017cytkownik jest zobowi\u0105zany do ochrony swojego has\u0142a. Has\u0142o ma charakter poufny. Zabronione jest udost\u0119pnianie ujawnianie has\u0142a w jakikolwiek spos\u00f3b (np. przekazanie innej osobie, zapisywanie w formie nie szyfrowanej, pozostawianie na widoku). W przypadku podejrzenia ujawnienia has\u0142a u\u017cytkownik zobowi\u0105zany jest do jego natychmiastowej zmiany oraz przekazania informacji o zdarzeniu do administrator\u00f3w klastra oraz Dzia\u0142u IT CeNT UW. U\u017cytkownik ponosi odpowiedzialno\u015b\u0107 za wszelkie czynno\u015bci wykonane za po\u015brednictwem konta do niego przypisanego zabezpieczonego jego has\u0142em. Tworz\u0105c has\u0142o nale\u017cy unika\u0107: przyjmowania regularnych schemat\u00f3w w tworzeniu hase\u0142, wybierania sekwencji b\u0105d\u017a powt\u00f3rze\u0144 znak\u00f3w \u0142atwych do podejrzenia, korzystania z nazwy w\u0142asnej u\u017cytkownika, wa\u017cnych dla u\u017cytkownika dat, imion, numer\u00f3w telefon\u00f3w kom\u00f3rkowych, numer\u00f3w rejestracyjnych aut. Zasady tworzenia hase\u0142: Has\u0142o powinno mie\u0107 co najmniej 10 znak\u00f3w (zalecane minimum 15), has\u0142o powinno zawiera\u0107 wielkie i ma\u0142e litery, znaki specjalne, cyfry (0 - 9). Zabrania si\u0119: podawania has\u0142a w wiadomo\u015bciach e-mail, b\u0105d\u017a te\u017c w odpowiedzi na \u017c\u0105danie, kt\u00f3re zosta\u0142o przes\u0142ane poczt\u0105 e-mail, wpisywania hase\u0142 do komputer\u00f3w powstaj\u0105cych w u\u017cytku publicznym np. komputery w kafejkach internetowych, hotelach, bibliotekach itd, zapisywania hase\u0142 w plikach niezaszyfrowanych. U\u017cytkownik zobligowany jest do zmiany has\u0142a nadanego przez administratora przy pierwszej pr\u00f3bie logowania. Zmiana has\u0142a dost\u0119powego do w\u0119z\u0142a dost\u0119powego wymuszana jest co 90 dni. Informacje o utracie has\u0142a, konieczno\u015bci jego zresetowania oraz inne problemy techniczne powinny by\u0107 zg\u0142aszane na adres dzia\u0142u IT CeNT UW - it @ cent.uw.edu.pl do\u0142\u0105czaj\u0105c w CC administrator\u00f3w klasta. Temat wiadomo\u015bci powinien rozpoczyna\u0107 si\u0119 od prefiksu [sih-61] .","title":"General rules"},{"location":"running_jobs/","text":"Introduction Jobs (both batch and interactive sessions) on EDI should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - cpu and gpu . GPU partition has higher priority. No limits are currently enforced on the time of execution. Constraints ( rtx2080 , gtx1080 ) can be used to select certain GPU architectures. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 gpu interact username R 11 -18:07:21 1 edi08 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cpu up infinite 1 drain* edi03 cpu up infinite 1 mix edi08 cpu up infinite 7 idle edi [ 00 -02,04-07 ] gpu up infinite 1 drain* edi03 gpu up infinite 1 mix edi08 gpu up infinite 6 idle edi [ 01 -02,04-07 ] Interactive sessions EDI is commonly used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. To facilitate allocating resources for interactive sessions a convenient wrapper ( alloc ) has been prepared. You can tweak your allocation depending on work needs, see the following table for details and examples. alloc options are as follows: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) -g Number of GPUs allocated for the job ( default = 0, max = 2 ) -m Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Host to start your session ( default = host you are running alloc on ) Example To obtain an allocation on edi02 with 1 gpu and 6 cores and a total of 12 GB of memory : alloc -n 6 -w edi02 -g 1 -m 2 Important! Please remember to quit your interactive allocation when you're done with your work. Batch jobs Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p gpu # GPU partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Running calculations"},{"location":"running_jobs/#introduction","text":"Jobs (both batch and interactive sessions) on EDI should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - cpu and gpu . GPU partition has higher priority. No limits are currently enforced on the time of execution. Constraints ( rtx2080 , gtx1080 ) can be used to select certain GPU architectures. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 gpu interact username R 11 -18:07:21 1 edi08 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cpu up infinite 1 drain* edi03 cpu up infinite 1 mix edi08 cpu up infinite 7 idle edi [ 00 -02,04-07 ] gpu up infinite 1 drain* edi03 gpu up infinite 1 mix edi08 gpu up infinite 6 idle edi [ 01 -02,04-07 ]","title":"Introduction"},{"location":"running_jobs/#interactive-sessions","text":"EDI is commonly used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. To facilitate allocating resources for interactive sessions a convenient wrapper ( alloc ) has been prepared. You can tweak your allocation depending on work needs, see the following table for details and examples. alloc options are as follows: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) -g Number of GPUs allocated for the job ( default = 0, max = 2 ) -m Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Host to start your session ( default = host you are running alloc on ) Example To obtain an allocation on edi02 with 1 gpu and 6 cores and a total of 12 GB of memory : alloc -n 6 -w edi02 -g 1 -m 2 Important! Please remember to quit your interactive allocation when you're done with your work.","title":"Interactive sessions"},{"location":"running_jobs/#batch-jobs","text":"Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p gpu # GPU partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Batch jobs"},{"location":"software/","text":"Name Version Hosts Location Maintainer amber (with ambertools) 18 all /opt/apps/amber18/* KK-SDH amber (with ambertools) 20 all /opt/apps/amber20/* KK-SDH blast-legacy 2.2.9 all /opt/apps/blast-legacy/bin/* KK-SDH cd-hit 4.8.1 all /opt/apps/cd-hit KK-SDH ccmpred 688e4ba all /opt/apps/CCMpred/bin/ccmpred KK-SDH clustalomega 1.2.4 all /opt/apps/clustalo/clustalo-1.2.4 KK-SDH dssp 3.1.4 all /opt/apps/dssp-3.1.4/mkdssp Binary erroneously shows version 3.1.2 KK-SDH dssp 2.3.0 all /opt/apps/dssp-2.3.0/mkdssp This version works with Socket KK-SDH hh-suite 3.3.0 all /opt/apps/hh-suite/bin/* KK-SDH hmmer 3.3.2 all /opt/apps/hmmer-3.3.2/* KK-SDH mafft 7.490 all /opt/apps/mafft/bin/* KK-SDH master 1.5.1 all /opt/apps/master/bin/* KK-SDH maxcluster 0.6.6 all /opt/apps//maxcluster/maxcluster64bit KK-SDH mmseqs2 9-d36de all /opt/apps/MMseqs2/bin/* KK-SDH modeller 9.2.5 all mod9.25 (in $PATH ) SDH muscle 3.8.31 all /opt/apps/muscle/muscle3.8.31 KK-SDH ncbi-blast 2.9.0 all /opt/apps/ncbi-blast+/bin/* KK-SDH ont-guppy 6.0.1+652ffd1 edi0[1-8] /opt/apps/ont-guppy/bin/* KK-SDH pd2_public 5b84579 all /opt/apps/pd2_public KK-SDH pymol 2.3.0 all /opt/apps/pymol/bin/pymol KK-SDH rosetta 3.10 all /opt/apps/rosetta-3.10/main/source/bin/* KK-SDH rosetta 3.12 all /opt/apps/rosetta-3.12/main/source/bin/* KK-SDH socket 3.03 all /opt/apps/socket/socket KK-SDH socket2 all all /opt/apps/socket2/socket2 Binary erroneously shows version 3.02 KK-SDH","title":"Available software"}]}