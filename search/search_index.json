{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation webpage of the EDI cluster. Section Description Getting started Basic information about the account setup and connections and data transfer. Running calculations Information on jobs scheduling, SLURM and best practices of interactive work. FAQ Frequently asked questions and solutions to common problems. Available resources Technical description of available resources. Available software List of preinstalled software packages available to all users.","title":"Home"},{"location":"faq/","text":"How to run Jupyter notebooks remotely on EDI? Assuming jupyter is running on edi07 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@edi07 Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888 How to install python packages on EDI? python3 (3.6.9) and python2 (2.7.17) along with the recent pip and virtualenv are installed system-wide on each EDI. You can simply install packages either with pip install --user or use virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory. I want to use program XXX on EDI, can I install it on my own? You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators. What are the guidelines for /home/nfs/ distributed filesystem use? Store only the necessary data in your /home/nfs/your_username directory. The space in this filesystem is limited. For example: storing your conda distribution, scripts, inputs for jobs run on the cluster and small outputs that you'll move to personal space after completion of the calculations is OK . Avoid prolonged and heavy I/O operations (like reading and writing large files). For example: Writing output from the MD simulations in amber (successive, small write operations) is OK . Reading a 100 GB file from nfs directory is not OK .","title":"FAQ"},{"location":"faq/#how-to-run-jupyter-notebooks-remotely-on-edi","text":"Assuming jupyter is running on edi07 (port 8888 ) and that the connection is established with sshuttle (see Getting started section for more details) it is possible setup the tunnel as follows: ssh -NL 8888 :localhost:8888 your_username@edi07 Afterwards you should be able to see the running Jupyter instance via browser at the URL: http://localhost:8888","title":"How to run Jupyter notebooks remotely on EDI?"},{"location":"faq/#how-to-install-python-packages-on-edi","text":"python3 (3.6.9) and python2 (2.7.17) along with the recent pip and virtualenv are installed system-wide on each EDI. You can simply install packages either with pip install --user or use virtualenv (this will allow to handle multiple projects with possibly conflicting dependencies). Finally, if you need newer versions of python or want to handle complicated dependencies you can install a local version of anaconda in your $HOME directory.","title":"How to install python packages on EDI?"},{"location":"faq/#i-want-to-use-program-xxx-on-edi-can-i-install-it-on-my-own","text":"You can install any software you like in your $HOME directory (as long as you have a valid license to use it). If you need support in setup of some program or want it to be installed system-wide, please contact the administrators.","title":"I want to use program XXX on EDI, can I install it on my own?"},{"location":"faq/#what-are-the-guidelines-for-homenfs-distributed-filesystem-use","text":"Store only the necessary data in your /home/nfs/your_username directory. The space in this filesystem is limited. For example: storing your conda distribution, scripts, inputs for jobs run on the cluster and small outputs that you'll move to personal space after completion of the calculations is OK . Avoid prolonged and heavy I/O operations (like reading and writing large files). For example: Writing output from the MD simulations in amber (successive, small write operations) is OK . Reading a 100 GB file from nfs directory is not OK .","title":"What are the guidelines for /home/nfs/ distributed filesystem use?"},{"location":"first_steps/","text":"Setting up an account In order to create your an account on EDI cluster please reach out to Janek and / or Staszek via Slack. You will get a set of two credentials in different e-mail messages. First password allows to login to the entry node (jumphost) at lbs.cent.uw.edu.pl . This password needs to be changed after first login and subsequently in 90 days intervals. Second password allows to login to the compute nodes edi0[0-8] . Information Please note that for now password changes on each of the compute nodes and the entry node are not synced. The centralized authentication system will be introduced in near future. Connecting via SSH Connections to the EDI cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@lbs.cent.uw.edu.pl This will bring you to the entry node (jumphost) , afterwards you can connect to any of the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@edi01 In order to simplify file copying, every day work with e.g. Jupyter notebooks the suggested way of connecting to individual edi nodes is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@lbs.cent.uw.edu.pl 10 .10.61.1/24 Once connection is established you can directly login to any edi node: ssh edi05 Information During the first connection to the entry node you will be required to change the initially obtained password. Information Additionally, depending on your computer and network settings, you may have to connect to edi nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here Work environment Each user has access to two personal directories: /home/users/your_username /home/nfs/your_username Warning The contents of the default directory /home/users/your_username are unique to each compute node and they are not available on other nodes. In order to use distributed file system please /home/nfs/your_username network mount (please follow the guidelines ). Transferring files The recommended options to send or fetch files from EDI cluster are either scp or rsync . The storage on the entry host lbs.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly. Information Assuming you established a connection with sshuttle you can directly send files or directories to any edi node: scp file.txt your_username@edi05: Next steps Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Getting started"},{"location":"first_steps/#setting-up-an-account","text":"In order to create your an account on EDI cluster please reach out to Janek and / or Staszek via Slack. You will get a set of two credentials in different e-mail messages. First password allows to login to the entry node (jumphost) at lbs.cent.uw.edu.pl . This password needs to be changed after first login and subsequently in 90 days intervals. Second password allows to login to the compute nodes edi0[0-8] . Information Please note that for now password changes on each of the compute nodes and the entry node are not synced. The centralized authentication system will be introduced in near future.","title":"Setting up an account"},{"location":"first_steps/#connecting-via-ssh","text":"Connections to the EDI cluster are handled via SSH protocol. See the figure below for a brief introduction of the network organization: In order to login to the entry node you can issue the following command: ssh your_username@lbs.cent.uw.edu.pl This will bring you to the entry node (jumphost) , afterwards you can connect to any of the compute nodes ( click here for a complete list of available resources), for example: ssh your_username@edi01 In order to simplify file copying, every day work with e.g. Jupyter notebooks the suggested way of connecting to individual edi nodes is to use sshuttle . This allows to bypass the login node and work almost the same way as being connected via VPN to the local network. Example Assuming sshuttle was installed according to the guide you can connect as follows: sshuttle --dns -NHr your_username@lbs.cent.uw.edu.pl 10 .10.61.1/24 Once connection is established you can directly login to any edi node: ssh edi05 Information During the first connection to the entry node you will be required to change the initially obtained password. Information Additionally, depending on your computer and network settings, you may have to connect to edi nodes once without sshuttle so that SSH connections are properly configured. To avoid putting password during each login you can set up authorization via a certificate - additional information is available here","title":"Connecting via SSH"},{"location":"first_steps/#work-environment","text":"Each user has access to two personal directories: /home/users/your_username /home/nfs/your_username Warning The contents of the default directory /home/users/your_username are unique to each compute node and they are not available on other nodes. In order to use distributed file system please /home/nfs/your_username network mount (please follow the guidelines ).","title":"Work environment"},{"location":"first_steps/#transferring-files","text":"The recommended options to send or fetch files from EDI cluster are either scp or rsync . The storage on the entry host lbs.cent.uw.edu.pl is very limited therefore it is recommended to setup sshuttle to send / fetch files directly. Information Assuming you established a connection with sshuttle you can directly send files or directories to any edi node: scp file.txt your_username@edi05:","title":"Transferring files"},{"location":"first_steps/#next-steps","text":"Once the basics are set up you should be able to start running calculations. Follow the next chapter for more details.","title":"Next steps"},{"location":"resources/","text":"EDI cluster has in total 92 physical CPUs (184 virtual), 16 GPUs, 576 GB of RAM and around 140 TB of storage. Characteristcs of available compute nodes are summarized below: Name CPU GPU RAM Storage edi00 Xeon Gold 6230 (20 cores) None 64 GB 8 x 6TB (RAID5) HDD 1 TB SSD edi0[1-3] i7-6950X (10 cores) 2x GeForce 1080 (8 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi0[4-7] i9-9900K (8 cores) 2x NVIDIA RTX 2080Ti (11 GB VRAM) 64 GB 4 x 6TB HDD (RAID5) edi08 i9-9900K (8 cores) 2x NVIDIA RTX A4500 (20 GB VRAM) 64 GB 4 x 6TB HDD (RAID5)","title":"Available resources"},{"location":"running_jobs/","text":"Introduction Jobs (both batch and interactive sessions) on EDI should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - cpu and gpu . GPU partition has higher priority. No limits are currently enforced on the time of execution. Constraints ( rtx2080 , gtx1080 ) can be used to select certain GPU architectures. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 gpu interact username R 11 -18:07:21 1 edi08 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cpu up infinite 1 drain* edi03 cpu up infinite 1 mix edi08 cpu up infinite 7 idle edi [ 00 -02,04-07 ] gpu up infinite 1 drain* edi03 gpu up infinite 1 mix edi08 gpu up infinite 6 idle edi [ 01 -02,04-07 ] Interactive sessions EDI is commonly used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. To facilitate allocating resources for interactive sessions a convenient wrapper ( alloc ) has been prepared. You can tweak your allocation depending on work needs, see the following table for details and examples. alloc options are as follows: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) -g Number of GPUs allocated for the job ( default = 0, max = 2 ) -m Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Host to start your session ( default = host you are running alloc on ) Example To obtain an allocation on edi02 with 1 gpu and 6 cores and a total of 12 GB of memory : alloc -n 6 -w edi02 -g 1 -m 2 Important! Please remember to quit your interactive allocation when you're done with your work. Batch jobs Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p gpu # GPU partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Running calculations"},{"location":"running_jobs/#introduction","text":"Jobs (both batch and interactive sessions) on EDI should be run through slurm resource manager. For the quick overview of slurm you can refer to the video: link Information Slurm details: Two partitions are available - cpu and gpu . GPU partition has higher priority. No limits are currently enforced on the time of execution. Constraints ( rtx2080 , gtx1080 ) can be used to select certain GPU architectures. Example To get the information on the currently running jobs run squeue : ~$ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 87719 gpu interact username R 11 -18:07:21 1 edi08 To get the information on the slurm partitions and their details run sinfo : ~$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cpu up infinite 1 drain* edi03 cpu up infinite 1 mix edi08 cpu up infinite 7 idle edi [ 00 -02,04-07 ] gpu up infinite 1 drain* edi03 gpu up infinite 1 mix edi08 gpu up infinite 6 idle edi [ 01 -02,04-07 ]","title":"Introduction"},{"location":"running_jobs/#interactive-sessions","text":"EDI is commonly used for interactive work with data, e.g. performing ad-hoc analyses and visualizations with python and jupyter-notebooks. To facilitate allocating resources for interactive sessions a convenient wrapper ( alloc ) has been prepared. You can tweak your allocation depending on work needs, see the following table for details and examples. alloc options are as follows: Argument Description -n Number of cores used allocated for the job ( default = 1, max = 36 ) -g Number of GPUs allocated for the job ( default = 0, max = 2 ) -m Amount of memory (in GBs) per allocated core allocated for the job ( default = 1, max = 60 ) -w Host to start your session ( default = host you are running alloc on ) Example To obtain an allocation on edi02 with 1 gpu and 6 cores and a total of 12 GB of memory : alloc -n 6 -w edi02 -g 1 -m 2 Important! Please remember to quit your interactive allocation when you're done with your work.","title":"Interactive sessions"},{"location":"running_jobs/#batch-jobs","text":"Longer, resource demanding jobs typically should be scheduled in SLURM batch mode. Below you can find the example of the SLURM batch script that you can use to schedule a job: Example Suppose the following job.sh batch file: #!/bin/bash #SBATCH -p gpu # GPU partition #SBATCH -n 8 # 8 cores #SBATCH --gres=gpu:1 # 1 GPU #SBATCH --mem=30GB # 30 GB of RAM #SBATCH -J job_name # name of your job your_program -i input_file -o output_path You can submit the specified job via sbatch command: ~$ sbatch job.sh Submitted batch job 1234","title":"Batch jobs"},{"location":"software/","text":"Name Version Hosts Location Maintainer amber (with ambertools) 18 all /opt/apps/amber18/* JL amber (with ambertools) 20 all /opt/apps/amber20/* JL blast-legacy 2.2.9 all /opt/apps/blast-legacy/bin/* JL cd-hit 4.8.1 all /opt/apps/cd-hit JL ccmpred 688e4ba all /opt/apps/CCMpred/bin/ccmpred JL clustalomega 1.2.4 all /opt/apps/clustalo/clustalo-1.2.4 JL dssp 3.1.4 all /opt/apps/dssp-3.1.4/mkdssp Binary erroneously shows version 3.1.2 JL dssp 2.3.0 all /opt/apps/dssp-2.3.0/mkdssp This version works with Socket JL hh-suite 3.3.0 all /opt/apps/hh-suite/bin/* JL hmmer 3.3.2 all /opt/apps/hmmer-3.3.2/* JL mafft 7.490 all /opt/apps/mafft/bin/* JL master 1.5.1 all /opt/apps/master/bin/* JL maxcluster 0.6.6 all /opt/apps//maxcluster/maxcluster64bit JL mmseqs2 9-d36de all /opt/apps/MMseqs2/bin/* JL modeller 9.2.5 all mod9.25 (in $PATH ) SDH muscle 3.8.31 all /opt/apps/muscle/muscle3.8.31 JL ncbi-blast 2.9.0 all /opt/apps/ncbi-blast+/bin/* JL ont-guppy 6.0.1+652ffd1 edi0[1-8] /opt/apps/ont-guppy/bin/* JL pd2_public 5b84579 all /opt/apps/pd2_public JL pymol 2.3.0 all /opt/apps/pymol/bin/pymol JL rosetta 3.10 all /opt/apps/rosetta-3.10/main/source/bin/* JL rosetta 3.12 all /opt/apps/rosetta-3.12/main/source/bin/* JL socket 3.03 all /opt/apps/socket/socket JL socket2 all all /opt/apps/socket2/socket2 Binary erroneously shows version 3.02 JL","title":"Available software"}]}